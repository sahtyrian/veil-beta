# VEIL — Virtual Experiential Interactive Landscape  
**BETA v1.0**

VEIL is a deterministic Audio-Visual Computation System for Multi-Modal Spatial Interpretation of Temporal Wave Input developed by Cassidy Howell (Sahtyre) under Liquid System Dynamics.

It explores the relationship between music, geometry, motion, and perception through real-time computation.

This repository contains the **VEIL BETA v1.0** reference implementation.

---

## Overview

VEIL transforms audio into two interconnected visual modes:

- **Neural Map** — a micro-level network derived from audio features  
- **Neural Galaxy** — a macro-level spatial projection generated from the same underlying audio DNA  

The system is designed so that:
- The same audio input always produces the same structural result  
- Visual modes can change without interrupting audio playback  
- Sound interpretation and sound playback are cleanly separated  

VEIL is intended as a foundation for further exploration in immersive media, interactive art, and experiential sound visualization.

---

## Core Concepts

- **Deterministic Audio DNA**  
  Audio is analyzed and converted into a stable internal representation.  
  This ensures repeatable structure across sessions and modes.

- **Single Global Audio Transport**  
  Audio playback is centralized and independent from visual interpretation.

- **Mode-Based Visual Interpretation**  
  Multiple visual systems interpret the same audio source without altering it.

- **Real-Time Reactivity**  
  Visual elements respond continuously to frequency, amplitude, and temporal changes in the audio signal.

  VEIL is designed as a reusable computational substrate, not a fixed visual style.

---

## Status

This release is considered **BETA v1.0**.

The system is stable, functional, and architecturally complete at a foundational level.  
Some controls and features are intentionally deferred for future versions.

---

## Usage

VEIL runs entirely in the browser using modern Web APIs.

Typical workflow:
1. Load the application
2. Upload an audio file
3. Explore the audio through Neural Map and Neural Galaxy modes

No server or backend is required.

---

## Project Structure

